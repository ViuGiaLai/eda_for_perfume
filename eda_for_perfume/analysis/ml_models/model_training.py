from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import joblib
import numpy as np
import pandas as pd
import os

class ImprovedModelTrainer:
    """
    âœ… MODEL TRAINING Cáº¢I TIáº¾N vá»›i:
    - ThÃªm ElasticNet, ExtraTreesRegressor
    - Hyperparameter tuning tá»‘t hÆ¡n
    - Validation strategy cháº·t cháº½ hÆ¡n
    - MAPE tÃ­nh Ä‘Ãºng (loáº¡i bá» y=0)
    """
    
    def __init__(self, use_log_target=True):
        self.use_log_target = use_log_target
        self.models = {
            'LinearRegression': LinearRegression(),
            
            'Ridge': Ridge(
                alpha=50.0,  # TÄƒng tá»« 10 â†’ 50 Ä‘á»ƒ regularization máº¡nh hÆ¡n
                random_state=42
            ),
            
            'Lasso': Lasso(
                alpha=5.0,  # TÄƒng tá»« 1 â†’ 5
                random_state=42,
                max_iter=5000
            ),
            
            'ElasticNet': ElasticNet(  # âœ… NEW MODEL
                alpha=5.0,
                l1_ratio=0.5,  # Mix L1 + L2
                random_state=42,
                max_iter=5000
            ),
            
            'DecisionTree': DecisionTreeRegressor(
                max_depth=8,  # Giáº£m tá»« 10 â†’ 8 Ä‘á»ƒ trÃ¡nh overfit
                min_samples_split=10,  # TÄƒng tá»« 2 â†’ 10
                min_samples_leaf=5,  # ThÃªm constraint
                random_state=42
            ),
            
            'RandomForest': RandomForestRegressor(
                n_estimators=200,  # Sá»‘ cÃ¢y Ä‘á»§ lá»›n
                max_depth=8,  # Giáº£m sÃ¢u cÃ¢y Ä‘á»ƒ giáº£m overfit
                min_samples_split=20,  # YÃªu cáº§u nhiá»u máº«u hÆ¡n trÆ°á»›c khi split
                min_samples_leaf=8,  # LÃ¡ pháº£i cÃ³ nhiá»u máº«u hÆ¡n
                max_features='sqrt',  # Háº¡n cháº¿ sá»‘ feature má»—i split
                max_samples=0.8,  # Chá»‰ dÃ¹ng 80% máº«u cho má»—i cÃ¢y
                random_state=42,
                n_jobs=-1
            ),
            
            'ExtraTrees': ExtraTreesRegressor(  # âœ… NEW MODEL
                n_estimators=200,
                max_depth=8,
                min_samples_split=20,
                min_samples_leaf=8,
                max_features='sqrt',
                max_samples=0.8,
                random_state=42,
                n_jobs=-1
            ),
            
            'GradientBoosting': GradientBoostingRegressor(
                n_estimators=200,  # Nhiá»u cÃ¢y hÆ¡n nhÆ°ng má»—i bÆ°á»›c há»c cháº­m hÆ¡n
                max_depth=3,  # CÃ¢y nÃ´ng hÆ¡n Ä‘á»ƒ bá»›t overfit
                learning_rate=0.03,  # Há»c cháº­m hÆ¡n Ä‘á»ƒ tá»•ng thá»ƒ mÆ°á»£t hÆ¡n
                min_samples_split=20,
                min_samples_leaf=8,
                subsample=0.7,  # DÃ¹ng 70% máº«u má»—i cÃ¢y Ä‘á»ƒ tÄƒng regularization
                random_state=42
            )
        }
        
        self.best_model = None
        self.best_model_name = None
        self.results = []
        
    def calculate_mape(self, y_true, y_pred):
        """
        âœ… TÃNH MAPE ÄÃšNG
        Loáº¡i bá» cÃ¡c giÃ¡ trá»‹ y_true = 0 Ä‘á»ƒ trÃ¡nh division by zero
        """
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        
        # Chá»‰ tÃ­nh MAPE trÃªn cÃ¡c giÃ¡ trá»‹ > 0
        mask = y_true > 0
        
        if mask.sum() == 0:
            return 999.99  # KhÃ´ng cÃ³ giÃ¡ trá»‹ nÃ o > 0
        
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        
        # Cap á»Ÿ 999% Ä‘á»ƒ trÃ¡nh sá»‘ quÃ¡ lá»›n
        return min(mape, 999.99)
    
    def train_all_models(self, X, y, test_size=0.2, cv_folds=5):
        """
        âœ… TRAIN Táº¤T Cáº¢ MODELS Vá»šI VALIDATION STRATEGY Tá»T HÆ N
        """
        print("\n" + "="*80)
        print("ğŸš€ Báº®T Äáº¦U TRAINING MODELS (IMPROVED VERSION)")
        print("="*80)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, shuffle=True
        )
        
        print(f"\nğŸ“Š Chia dá»¯ liá»‡u:")
        print(f"   â€¢ Train set: {X_train.shape[0]} samples")
        print(f"   â€¢ Test set: {X_test.shape[0]} samples")
        print(f"   â€¢ Features: {X_train.shape[1]}")
        
        # Setup cross-validation
        kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
        
        best_score = -np.inf
        
        for name, model in self.models.items():
            print(f"\n{'='*60}")
            print(f"ğŸ“ Training: {name}")
            print(f"{'='*60}")
            
            try:
                # Train
                model.fit(X_train, y_train)
                
                # Predict
                y_train_pred = model.predict(X_train)
                y_test_pred = model.predict(X_test)
                
                # âœ… METRICS ÄÃšNG
                train_r2 = r2_score(y_train, y_train_pred)
                test_r2 = r2_score(y_test, y_test_pred)
                
                train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
                test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
                
                train_mae = mean_absolute_error(y_train, y_train_pred)
                test_mae = mean_absolute_error(y_test, y_test_pred)
                
                # âœ… MAPE Ä‘Ãºng (loáº¡i bá» y=0)
                test_mape = self.calculate_mape(y_test, y_test_pred)
                train_mape = self.calculate_mape(y_train, y_train_pred)
                
                # Cross-validation vá»›i KFold
                cv_scores = cross_val_score(
                    model, X_train, y_train, 
                    cv=kfold, 
                    scoring='r2', 
                    n_jobs=-1
                )
                
                # âœ… OVERFIT GAP Ä‘Ãºng
                overfit_gap = abs(train_r2 - test_r2)
                
                # Save results
                result = {
                    'Model': name,
                    'Train_R2': train_r2,
                    'Test_R2': test_r2,
                    'Train_RMSE': train_rmse,
                    'Test_RMSE': test_rmse,
                    'Train_MAE': train_mae,
                    'Test_MAE': test_mae,
                    'Train_MAPE': train_mape,
                    'Test_MAPE': test_mape,
                    'CV_Mean': cv_scores.mean(),
                    'CV_Std': cv_scores.std(),
                    'CV_Min': cv_scores.min(),
                    'CV_Max': cv_scores.max(),
                    'Overfit_Gap': overfit_gap
                }
                
                self.results.append(result)
                
                # Print metrics
                print(f"\nğŸ“Š Performance Metrics:")
                print(f"   â€¢ Train RÂ²: {train_r2:.4f}")
                print(f"   â€¢ Test RÂ²:  {test_r2:.4f}")
                print(f"   â€¢ Overfit Gap: {overfit_gap:.4f}", 
                      "âœ…" if overfit_gap < 0.1 else "âš ï¸" if overfit_gap < 0.2 else "âŒ")
                
                print(f"\n   â€¢ Train RMSE: {train_rmse:.2f}")
                print(f"   â€¢ Test RMSE:  {test_rmse:.2f}")
                
                print(f"\n   â€¢ Train MAE: {train_mae:.2f}")
                print(f"   â€¢ Test MAE:  {test_mae:.2f}")
                
                print(f"\n   â€¢ Train MAPE: {train_mape:.2f}%", 
                      "âœ…" if train_mape < 20 else "âš ï¸" if train_mape < 50 else "âŒ")
                print(f"   â€¢ Test MAPE:  {test_mape:.2f}%", 
                      "âœ…" if test_mape < 20 else "âš ï¸" if test_mape < 50 else "âŒ")
                
                print(f"\n   â€¢ CV Score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
                print(f"   â€¢ CV Range: [{cv_scores.min():.4f}, {cv_scores.max():.4f}]")
                
                # âœ… CHá»ŒN MODEL Tá»T NHáº¤T Dá»±A trÃªn Test RÂ² vÃ  Overfit Gap
                # Æ¯u tiÃªn model cÃ³ Test RÂ² cao vÃ  Overfit Gap tháº¥p
                # score = test_r2 - (overfit_gap * 0.5)  # Penalty cho overfit
                
                # âœ…CHá»ŒN MODEL Tá»T NHáº¤T Dá»±A trÃªn Test MAPE + Test RÂ² + Overfit Gap
                # - Æ¯u tiÃªn MAPE tháº¥p (quan trá»ng nháº¥t)
                # - Váº«n khuyáº¿n khÃ­ch Test RÂ² cao
                # - Pháº¡t mÃ´ hÃ¬nh overfit (Overfit_Gap lá»›n)
                score = -(test_mape / 100.0) + 0.2 * test_r2 - 0.5 * overfit_gap

                if score > best_score:
                    best_score = score
                    self.best_model = model
                    self.best_model_name = name
                    print(f"\n   ğŸ† NEW BEST MODEL! (Score: {score:.4f})")
                    
            except Exception as e:
                print(f"\n   âŒ Error training {name}: {str(e)}")
                continue
        
        # ========================================
        # COMPARISON TABLE
        # ========================================
        print("\n" + "="*80)
        print("ğŸ“Š Báº¢NG SO SÃNH Táº¤T Cáº¢ MODELS")
        print("="*80)
        
        results_df = pd.DataFrame(self.results)
        
        # Sort by Test RÂ² descending
        results_df = results_df.sort_values('Test_R2', ascending=False)
        
        # Format for display
        display_df = results_df.copy()
        for col in ['Train_R2', 'Test_R2', 'CV_Mean']:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.4f}")
        for col in ['Train_RMSE', 'Test_RMSE', 'Train_MAE', 'Test_MAE']:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.2f}")
        for col in ['Train_MAPE', 'Test_MAPE', 'CV_Std', 'Overfit_Gap']:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.3f}")
        
        print(display_df.to_string(index=False))
        
        # ========================================
        # BEST MODEL SUMMARY
        # ========================================
        print("\n" + "="*80)
        print("ğŸ† MODEL Tá»T NHáº¤T")
        print("="*80)
        
        best_result = results_df[results_df['Model'] == self.best_model_name].iloc[0]
        
        print(f"\nğŸ¯ Model: {self.best_model_name}")
        print(f"\nğŸ“Š Test Set Performance:")
        print(f"   â€¢ RÂ² Score: {best_result['Test_R2']:.4f}")
        print(f"   â€¢ RMSE: {best_result['Test_RMSE']:.2f}")
        print(f"   â€¢ MAE: {best_result['Test_MAE']:.2f}")
        print(f"   â€¢ MAPE: {best_result['Test_MAPE']:.2f}%")
        
        print(f"\nğŸ”„ Cross-Validation:")
        print(f"   â€¢ Mean RÂ²: {best_result['CV_Mean']:.4f}")
        print(f"   â€¢ Std: Â±{best_result['CV_Std']:.4f}")
        
        print(f"\nâš–ï¸  Overfit Assessment:")
        print(f"   â€¢ Gap: {best_result['Overfit_Gap']:.4f}")
        if best_result['Overfit_Gap'] < 0.1:
            print(f"   â€¢ Status: âœ… CÃ¢n báº±ng tá»‘t")
        elif best_result['Overfit_Gap'] < 0.2:
            print(f"   â€¢ Status: âš ï¸  HÆ¡i overfit")
        else:
            print(f"   â€¢ Status: âŒ Overfit nghiÃªm trá»ng")
        
        print("="*80)
        
        return self.best_model, results_df
    
    def get_feature_importance(self, X, top_n=20):
        """Láº¥y top features quan trá»ng nháº¥t"""
        if self.best_model_name in ['RandomForest', 'GradientBoosting', 
                                     'DecisionTree', 'ExtraTrees']:
            importances = self.best_model.feature_importances_
            feature_importance = pd.DataFrame({
                'Feature': X.columns,
                'Importance': importances
            }).sort_values('Importance', ascending=False).head(top_n)
            
            print(f"\n" + "="*80)
            print(f"ğŸ” TOP {top_n} FEATURES QUAN TRá»ŒNG NHáº¤T ({self.best_model_name})")
            print("="*80)
            
            for idx, row in feature_importance.iterrows():
                bar_length = int(row['Importance'] * 50)
                bar = 'â–ˆ' * bar_length
                print(f"   {row['Feature'][:40]:<40} {bar} {row['Importance']:.4f}")
            
            print("="*80)
            
            return feature_importance
        else:
            print(f"\nâš ï¸  Model {self.best_model_name} khÃ´ng há»— trá»£ feature importance")
            return None
    
    def save_model(self, filepath='data/models/perfume_sales_model.pkl'):
        """LÆ°u model Ä‘Ã£ train"""
        if not os.path.isabs(filepath):
            filepath = os.path.join(os.path.dirname(__file__), '..', '..', filepath)
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        joblib.dump(self.best_model, filepath)
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u model táº¡i: {filepath}")
    
    @staticmethod
    def load_model(filepath='data/models/perfume_sales_model.pkl'):
        """Load model Ä‘Ã£ train"""
        if not os.path.isabs(filepath):
            filepath = os.path.join(os.path.dirname(__file__), '..', '..', filepath)
        
        return joblib.load(filepath)