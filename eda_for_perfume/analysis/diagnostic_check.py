"""
Script kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u vÃ  Ä‘á» xuáº¥t cáº£i thiá»‡n
Cháº¡y: python manage.py shell < diagnostic_check.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from pathlib import Path

# Set up paths
BASE_DIR = Path(__file__).resolve().parent
DATA_PATH = os.path.join(BASE_DIR, 'ebay_mens_perfume.csv')

def load_data():
    """Load data from CSV file"""
    try:
        df = pd.read_csv(DATA_PATH)
        print(f"âœ… ÄÃ£ load {len(df)} dÃ²ng dá»¯ liá»‡u tá»« {DATA_PATH}")
        return df
    except Exception as e:
        print(f"âŒ Lá»—i khi load dá»¯ liá»‡u: {e}")
        return None

def analyze_target_variable(df, target_col='sold'):
    """PhÃ¢n tÃ­ch biáº¿n má»¥c tiÃªu (sold)"""
    print("\n" + "="*80)
    print("ğŸ“Š PHÃ‚N TÃCH BIáº¾N Má»¤C TIÃŠU (SOLD)")
    print("="*80)
    
    if target_col not in df.columns:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t '{target_col}' trong dá»¯ liá»‡u")
        return
    
    # Basic statistics
    sold = df[target_col]
    print(f"\nğŸ“ˆ Thá»‘ng kÃª cÆ¡ báº£n:")
    print(sold.describe())
    
    # Missing values
    missing = sold.isnull().sum()
    print(f"\nğŸ” GiÃ¡ trá»‹ thiáº¿u: {missing} ({missing/len(sold)*100:.1f}%)")
    
    # Zero values
    zero_count = (sold == 0).sum()
    print(f"\nğŸ”¢ Sá»‘ lÆ°á»£ng giÃ¡ trá»‹ 0: {zero_count} ({zero_count/len(sold)*100:.1f}%)")
    
    # Distribution analysis
    print("\nğŸ“Š PhÃ¢n vá»‹:")
    percentiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
    for p in percentiles:
        val = sold.quantile(p)
        print(f"  {int(p*100)}%: {val:.1f}")
    
    # Outliers detection
    q1 = sold.quantile(0.25)
    q3 = sold.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    
    outliers = df[(sold < lower_bound) | (sold > upper_bound)]
    print(f"\nâš ï¸  PhÃ¡t hiá»‡n {len(outliers)} outliers (theo phÆ°Æ¡ng phÃ¡p IQR)")
    
    # Distribution plot
    plt.figure(figsize=(12, 6))
    sns.histplot(sold, kde=True, bins=50)
    plt.title('PhÃ¢n bá»‘ cá»§a biáº¿n má»¥c tiÃªu (sold)')
    plt.xlabel('Sá»‘ lÆ°á»£ng Ä‘Ã£ bÃ¡n')
    plt.ylabel('Táº§n suáº¥t')
    plt.grid(True)
    
    # Save plot
    plot_path = os.path.join(BASE_DIR, 'static', 'analysis', 'sold_distribution.png')
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    plt.savefig(plot_path)
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ phÃ¢n bá»‘ táº¡i: {plot_path}")
    plt.close()

def analyze_features(df, target_col='sold'):
    """PhÃ¢n tÃ­ch cÃ¡c Ä‘áº·c trÆ°ng"""
    print("\n" + "="*80)
    print("ğŸ” PHÃ‚N TÃCH CÃC Äáº¶C TRÆ¯NG")
    print("="*80)
    
    # Check for missing values
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100
    missing_df = pd.DataFrame({'missing_count': missing, 'missing_percentage': missing_pct})
    missing_df = missing_df[missing_df['missing_count'] > 0].sort_values('missing_count', ascending=False)
    
    if not missing_df.empty:
        print("\nâŒ CÃ¡c cá»™t bá»‹ thiáº¿u dá»¯ liá»‡u:")
        print(missing_df)
    else:
        print("\nâœ… KhÃ´ng cÃ³ dá»¯ liá»‡u bá»‹ thiáº¿u")
    
    # Analyze numerical features
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    if target_col in numeric_cols:
        numeric_cols.remove(target_col)
    
    if numeric_cols:
        print("\nğŸ“ˆ Thá»‘ng kÃª cÃ¡c Ä‘áº·c trÆ°ng sá»‘:")
        print(df[numeric_cols].describe().T)
        # Correlation with target
        if target_col in df.columns:
            corr = df[numeric_cols + [target_col]].corr()[target_col].sort_values(ascending=False)
            print("\nğŸ“Š TÆ°Æ¡ng quan vá»›i biáº¿n má»¥c tiÃªu (sold):")
            print(corr)
            
            # Plot correlation heatmap
            plt.figure(figsize=(10, 8))
            sns.heatmap(df[numeric_cols + [target_col]].corr(), annot=True, cmap='coolwarm', center=0)
            plt.title('Ma tráº­n tÆ°Æ¡ng quan')
            
            # Save correlation plot
            corr_plot_path = os.path.join(BASE_DIR, 'static', 'analysis', 'correlation_heatmap.png')
            plt.savefig(corr_plot_path)
            print(f"\nğŸ’¾ ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ tÆ°Æ¡ng quan táº¡i: {corr_plot_path}")
            plt.close()
    
    # Analyze categorical features
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    if cat_cols:
        print("\nğŸ“Š Thá»‘ng kÃª cÃ¡c Ä‘áº·c trÆ°ng phÃ¢n loáº¡i:")
        for col in cat_cols:
            print(f"\nğŸ”¤ {col}:")
            print(f"Sá»‘ lÆ°á»£ng giÃ¡ trá»‹ duy nháº¥t: {df[col].nunique()}")
            print("GiÃ¡ trá»‹ phá»• biáº¿n:")
            print(df[col].value_counts().head())

def data_quality_assessment(df, target_col='sold'):
    """ÄÃ¡nh giÃ¡ tá»•ng quan cháº¥t lÆ°á»£ng dá»¯ liá»‡u"""
    print("\n" + "="*80)
    print("ğŸ† ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG Dá»® LIá»†U")
    print("="*80)
    
    issues = []
    warnings = []
    
    # 1. Check sample size
    if len(df) < 500:
        warnings.append(f"âš ï¸  KÃ­ch thÆ°á»›c máº«u nhá» ({len(df)} dÃ²ng), cÃ³ thá»ƒ khÃ´ng Ä‘á»§ Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh hiá»‡u quáº£")
    
    # 2. Check target variable
    if target_col in df.columns:
        # Check for class imbalance (for classification)
        if df[target_col].nunique() < 10:  # Assuming classification if few unique values
            class_dist = df[target_col].value_counts(normalize=True)
            if (class_dist < 0.1).any():
                issues.append(f"âŒ Máº¥t cÃ¢n báº±ng lá»›p nghiÃªm trá»ng: {class_dist.to_dict()}")
        
        # Check for zero-inflation (for regression)
        if df[target_col].nunique() > 10:  # Assuming regression if many unique values
            zero_count = (df[target_col] == 0).sum()
            if zero_count / len(df) > 0.3:
                issues.append(f"âŒ QuÃ¡ nhiá»u giÃ¡ trá»‹ 0 trong biáº¿n má»¥c tiÃªu ({zero_count/len(df)*100:.1f}%)")
    
    # 3. Check missing values
    missing_cols = df.isnull().sum()
    missing_cols = missing_cols[missing_cols > 0]
    if not missing_cols.empty:
        issues.append(f"âŒ CÃ³ {len(missing_cols)} cá»™t chá»©a giÃ¡ trá»‹ thiáº¿u")
    
    # 4. Check constant columns
    constant_cols = [col for col in df.columns if df[col].nunique() == 1]
    if constant_cols:
        issues.append(f"âŒ CÃ¡c cá»™t háº±ng sá»‘ (khÃ´ng cÃ³ thÃ´ng tin): {constant_cols}")
    
    # 5. Check duplicate rows
    if df.duplicated().sum() > 0:
        issues.append(f"âŒ PhÃ¡t hiá»‡n {df.duplicated().sum()} dÃ²ng trÃ¹ng láº·p")
    
    # Print results
    if issues:
        print("\nğŸš¨ Váº¤N Äá»€ Cáº¦N Xá»¬ LÃ:")
        for issue in issues:
            print(f"- {issue}")
    else:
        print("\nâœ… KhÃ´ng phÃ¡t hiá»‡n váº¥n Ä‘á» nghiÃªm trá»ng")
    
    if warnings:
        print("\nâš ï¸  Cáº¢NH BÃO:")
        for warning in warnings:
            print(f"- {warning}")

def generate_recommendations(df, target_col='sold'):
    """Táº¡o cÃ¡c khuyáº¿n nghá»‹ cáº£i thiá»‡n dá»¯ liá»‡u"""
    print("\n" + "="*80)
    print("ğŸ’¡ Äá»€ XUáº¤T Cáº¢I THIá»†N")
    print("="*80)
    
    print("\n1ï¸âƒ£  TIá»€N Xá»¬ LÃ Dá»® LIá»†U:")
    print("  - Xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u:")
    print("    * Sá»­ dá»¥ng giÃ¡ trá»‹ trung bÃ¬nh/trung vá»‹ cho biáº¿n sá»‘")
    print("    * Sá»­ dá»¥ng giÃ¡ trá»‹ phá»• biáº¿n nháº¥t cho biáº¿n phÃ¢n loáº¡i")
    print("    * Hoáº·c xÃ³a cÃ¡c dÃ²ng chá»©a giÃ¡ trá»‹ thiáº¿u náº¿u Ã­t")
    
    print("\n  - Xá»­ lÃ½ ngoáº¡i lai:")
    print("    * PhÃ¡t hiá»‡n vÃ  xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ ngoáº¡i lai báº±ng IQR hoáº·c Z-score")
    print("    * CÃ¢n nháº¯c sá»­ dá»¥ng log transformation cho biáº¿n lá»‡ch pháº£i")
    
    print("\n2ï¸âƒ£  Ká»¸ THUáº¬T Láº¤Y MáºªU:")
    if (df[target_col] == 0).mean() > 0.3:
        print("  - CÃ¢n nháº¯c sá»­ dá»¥ng ká»¹ thuáº­t láº¥y máº«u láº¡i (resampling):")
        print("    * Oversampling cho lá»›p thiá»ƒu sá»‘")
        print("    * Undersampling cho lá»›p Ä‘a sá»‘")
        print("    * SMOTE Ä‘á»ƒ táº¡o máº«u tá»•ng há»£p")
    
    print("\n3ï¸âƒ£  Ká»¸ THUáº¬T MÃƒ HÃ“A:")
    print("  - MÃ£ hÃ³a one-hot cho cÃ¡c biáº¿n phÃ¢n loáº¡i cÃ³ Ã­t giÃ¡ trá»‹ duy nháº¥t")
    print("  - Sá»­ dá»¥ng target encoding cho cÃ¡c biáº¿n phÃ¢n loáº¡i cÃ³ nhiá»u giÃ¡ trá»‹ duy nháº¥t")
    print("  - Chuáº©n hÃ³a (scale) cÃ¡c Ä‘áº·c trÆ°ng sá»‘ vá» cÃ¹ng má»™t khoáº£ng giÃ¡ trá»‹")
    
    print("\n4ï¸âƒ£  Ká»¸ THUáº¬T KHAI PHÃ Äáº¶C TRÆ¯NG:")
    print("  - Táº¡o cÃ¡c Ä‘áº·c trÆ°ng tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c biáº¿n")
    print("  - TrÃ­ch xuáº¥t thÃ´ng tin tá»« vÄƒn báº£n (náº¿u cÃ³)")
    print("  - Táº¡o cÃ¡c Ä‘áº·c trÆ°ng thá»‘ng kÃª theo nhÃ³m")
    
    print("\n5ï¸âƒ£  MÃ” HÃŒNH Dá»° ÄOÃN:")
    print("  - Thá»­ nghiá»‡m nhiá»u mÃ´ hÃ¬nh khÃ¡c nhau: XGBoost, LightGBM, Random Forest")
    print("  - Sá»­ dá»¥ng cross-validation Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t á»•n Ä‘á»‹nh")
    print("  - Tá»‘i Æ°u hyperparameters báº±ng GridSearch hoáº·c Bayesian Optimization")

def main():
    # Load data
    df = load_data()
    if df is None:
        return
    
    # Basic info
    print("\nğŸ“‹ THÃ”NG TIN CÆ  Báº¢N Vá»€ Dá»® LIá»†U")
    print("="*50)
    print(f"Tá»•ng sá»‘ dÃ²ng: {len(df)}")
    print(f"Tá»•ng sá»‘ cá»™t: {len(df.columns)}")
    print("\nCÃ¡c cá»™t trong dá»¯ liá»‡u:", ", ".join(df.columns.tolist()))
    
    # Perform analysis
    analyze_target_variable(df)
    analyze_features(df)
    data_quality_assessment(df)
    generate_recommendations(df)
    
    print("\nâœ… HoÃ n thÃ nh phÃ¢n tÃ­ch dá»¯ liá»‡u!")

if __name__ == "__main__":
    main()
